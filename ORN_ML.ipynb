{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "#Data Scaling/Encoding Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "#Data Split/Oversampling/Model Tuning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Models\n",
    "from sklearn import tree, svm, ensemble, linear_model\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "# Model Evaluation\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, roc_auc_score\n",
    "#Model/Pipeline Export\n",
    "import joblib\n",
    "#Nomogram\n",
    "from simpleNomo import nomogram\n",
    "#Dcurve\n",
    "from dcurves import dca, plot_graphs\n",
    "#SHAP\n",
    "import shap\n",
    "shap.initjs()\n",
    "#Random Seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and reformat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "df = pd.read_excel('./Data/Raw/ORN_Data.xlsx')\n",
    "#Subset to only include relevant rows\n",
    "df= df[:329]\n",
    "## Rename for interpretability\n",
    "df = df.rename(columns={'ORN': 'ORN_type', 'ORN ': 'ORN'})\n",
    "##Get relevant columns\n",
    "df_sub = df[[\n",
    "             'GENDER (0=female, 1=male)',\n",
    "             'AGE',\n",
    "             'REOPEN',\n",
    "             'ADMISSION( hospitalization days)',\n",
    "             'MEDEXPOSURE( treat plate exposure with medication only)',\n",
    "             'TXEXPOSURE(treatments other than medication, 1=plate removal, 2=another flap)',\n",
    "             'PRIOREX( prior opearion at same side, 0=no, 1=yes)',\n",
    "             'EXPOSUREFU( time from op to plate exposure)',\n",
    "             'OPTIME',\n",
    "             'ASA, 0= ASA I or II, 1=ASA III',\n",
    "             'BMI',\n",
    "             'DM(0=no, 1=yes)', \n",
    "             'RECUR(0=primary, 1=recurrence)', \n",
    "             'SECONDPRIMARY(0=no, 1=yes)',  \n",
    "             'SITE(1= mouth floor,  2=buccal, 3=retromolar, 4=gum, 5=tongue , 6=lip)', \n",
    "             'T(0=T1or2, 1=T3or4)', \n",
    "             'N(0= (-), 1=(+))',\n",
    "             'OVERALLSTAGE(1= stage1, 2=stage2/3, 3=stage4)',\n",
    "             'DEFECTTYPE(0= intraoral only, 1=composite defect)',\n",
    "             'JEWER(Jewer\\'s classification, 0=C, 1=L, 3=LC, 5=LCL)',\n",
    "             'LENGTH(defect length)',\n",
    "             'OSTEOTOMY(no. of osteostomy of the fibula bone)',\n",
    "             'PLATE(0=mini plate 1=reconstruction plate 2=preformed plate)',\n",
    "             'FLAP(0=OSC flap, 1=chimeric flap with muscle)',\n",
    "             'HGB(post-op hemoglobin)',\n",
    "             'ALB(post-op albumin)',\n",
    "             'BT(intra-op blood transfusion, 0=no, 1=yes)',\n",
    "             'ISCHEMICTIME',\n",
    "             'PRERT( pre-op RT)', \n",
    "             'POSTRT(post-op RT)', \n",
    "             'PRECT(pre-op chemotherapy)',\n",
    "             'POSTCT(post-op chemotherapy)', \n",
    "             'WOUNDINF(post-op wound infection)',\n",
    "             'EXPOSURE( plate exposure)',\n",
    "             'ORN'\n",
    "             ]]\n",
    "\n",
    "##Rename columns for readability\n",
    "df_renamed = df_sub.rename(columns = {\n",
    "    'REOPEN': 'REOPEN',\n",
    "    'ADMISSION( hospitalization days)': \"ADMISSION\",\n",
    "    'MEDEXPOSURE( treat plate exposure with medication only)': 'MEDEXPOSURE',\n",
    "    'TXEXPOSURE(treatments other than medication, 1=plate removal, 2=another flap)': \"TXEXPOSURE\",\n",
    "    'PRIOREX( prior opearion at same side, 0=no, 1=yes)': 'PRIOREX',\n",
    "    'EXPOSUREFU( time from op to plate exposure)': 'EXPOSUREFU',\n",
    "    'GENDER (0=female, 1=male)': 'GENDER',\n",
    "    'ASA, 0= ASA I or II, 1=ASA III': 'ASA',\n",
    "    'DM(0=no, 1=yes)': 'DM', \n",
    "    'RECUR(0=primary, 1=recurrence)': 'RECUR', \n",
    "    'SECONDPRIMARY(0=no, 1=yes)': 'SP',  \n",
    "    'SITE(1= mouth floor,  2=buccal, 3=retromolar, 4=gum, 5=tongue , 6=lip)': 'SITE', \n",
    "    'T(0=T1or2, 1=T3or4)': 'T', \n",
    "    'N(0= (-), 1=(+))': 'N',\n",
    "    'OVERALLSTAGE(1= stage1, 2=stage2/3, 3=stage4)': 'STAGE',\n",
    "    'DEFECTTYPE(0= intraoral only, 1=composite defect)': 'DEFECT_TYPE',\n",
    "    'JEWER(Jewer\\'s classification, 0=C, 1=L, 3=LC, 5=LCL)': 'JEWER',\n",
    "    'LENGTH(defect length)': 'LENGTH',\n",
    "    'OSTEOTOMY(no. of osteostomy of the fibula bone)': 'OSTEOTOMY',\n",
    "    'PLATE(0=mini plate 1=reconstruction plate 2=preformed plate)': 'PLATE',\n",
    "    'FLAP(0=OSC flap, 1=chimeric flap with muscle)': 'FLAP',\n",
    "    'HGB(post-op hemoglobin)': 'HGB',\n",
    "    'ALB(post-op albumin)': 'ALB',\n",
    "    'BT(intra-op blood transfusion, 0=no, 1=yes)': 'BT',\n",
    "    'PRERT( pre-op RT)': 'PRERT', \n",
    "    'POSTRT(post-op RT)': 'POSTRT', \n",
    "    'PRECT(pre-op chemotherapy)':'PRECT',\n",
    "    'POSTCT(post-op chemotherapy)': 'POSTCT', \n",
    "    'WOUNDINF(post-op wound infection)': 'WOUNDINF',\n",
    "    'EXPOSURE( plate exposure)':'EXPOSURE',\n",
    "})\n",
    "\n",
    "#Change the single 2 entry for exposure to a 1 for simplicity\n",
    "df_renamed.EXPOSURE = np.where(df_renamed.EXPOSURE >= 1, 1.0, df_renamed.EXPOSURE)\n",
    "\n",
    "#Extract features with NA or 999 entries\n",
    "for col in df_renamed.columns:\n",
    "    if col == 'OPTIME':\n",
    "        continue\n",
    "    na_vals = df_renamed[col].isna().sum()\n",
    "    nine_vals = (df_renamed[col] == 999).sum()\n",
    "    if na_vals > 0:\n",
    "        print(f'{col}: {na_vals}  NA vals')\n",
    "        print('--------')\n",
    "    elif(nine_vals > 0):\n",
    "        print(f'{col}: {nine_vals} 999 vals')\n",
    "        print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df_renamed[(df_renamed.ALB < 999.0)] ##Remove ALB na\n",
    "df_renamed = df_renamed[(df_renamed.BMI < 999.0)] ##Remove BMI na\n",
    "df_renamed = df_renamed[(df_renamed.ISCHEMICTIME < 999.0)] ##Remove ISCHEMICTIME na\n",
    "df_renamed = df_renamed[df_renamed.LENGTH > 0] ##Remove Length na\n",
    "#Ensure data is fully cleaned\n",
    "print(f'Total NA vals: {df_renamed.isna().sum().sum()}')\n",
    "print(f'Total 999 vals: {(df_renamed == 999).sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Val-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#65-35 train-test split with stratified distribution of ORN to No-ORN\n",
    "raw_X_train, raw_X_test, raw_y_train, raw_y_test = train_test_split(df_renamed.drop(['ORN'], axis = 1),\n",
    "                                                    df_renamed['ORN'], \n",
    "                                                    test_size=0.35,\n",
    "                                                    random_state = 42, \n",
    "                                                    stratify=df_renamed.ORN\n",
    "                                                    )\n",
    "\n",
    "#Reset index\n",
    "raw_X_train.reset_index(drop = True, inplace=True)\n",
    "raw_y_train.reset_index(drop = True, inplace=True)\n",
    "raw_X_test.reset_index(drop = True, inplace=True)\n",
    "raw_y_test.reset_index(drop = True, inplace=True)\n",
    "#Export raw train/test splits\n",
    "raw_X_train.to_excel('./Data/Raw/Raw_X_train.xlsx', index = False)\n",
    "raw_X_test.to_excel('./Data/Raw/Raw_X_test.xlsx', index = False)\n",
    "raw_y_train.to_excel('./Data/Raw/Raw_y_train.xlsx', index = False)\n",
    "raw_y_test.to_excel('./Data/Raw/Raw_y_test.xlsx', index = False)\n",
    "\n",
    "#Initialize feature lists\n",
    "binary_cols = [] #Binary categorical\n",
    "numerical_cols = [] #Numerical\n",
    "nominal_cols = [] #Nominal Multi-categorical\n",
    "ordinal_cols = ['STAGE'] #Ordinal categorical\n",
    "\n",
    "##Classify feature types\n",
    "for col in raw_X_train.columns:\n",
    "    if col == 'STAGE': #Stage is the only ordinal variable\n",
    "        continue\n",
    "    num_entries = len(raw_X_train[col].value_counts())\n",
    "    if num_entries == 2:\n",
    "        binary_cols.append(col)\n",
    "    elif num_entries > 10 or col == 'OSTEOTOMY': #Osteotomy only has 4 distinct entries but numerical\n",
    "        numerical_cols.append(col)\n",
    "    else:\n",
    "       nominal_cols.append(col)\n",
    "#Ensure total entries in lists add up\n",
    "len(binary_cols) + len(numerical_cols) + len(nominal_cols) + len(ordinal_cols) == raw_X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Model Pipeline\n",
    "numerical_transformer = Pipeline(steps=[ \n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "ml_categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop = 'first', sparse_output=False)), #Drop first col in one-hot encoding\n",
    "])\n",
    "ml_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols), #Scale numerical\n",
    "        ('cat', ml_categorical_transformer, nominal_cols) #Encode categorical\n",
    "    ],\n",
    "    remainder='passthrough' #Pass binary and ordinal through\n",
    ")\n",
    "ml_preprocessor.set_output(transform=\"pandas\") #Ensure output is df\n",
    "\n",
    "#Nomogram Pipeline\n",
    "nomo_categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(sparse_output=False)), #DONT drop first col in one-hot encoding\n",
    "])\n",
    "nomogram_preprocessor = ColumnTransformer( #No numerical scaling\n",
    "    transformers=[\n",
    "        ('cat', nomo_categorical_transformer, nominal_cols),\n",
    "    ],\n",
    "    remainder='passthrough' #Pass numerical, binary, and ordinal through\n",
    ") \n",
    "nomogram_preprocessor.set_output(transform=\"pandas\") #Ensure output is df\n",
    "\n",
    "###Fit on train data and transform both training and testing\n",
    "nomo_train_transformed = nomogram_preprocessor.fit_transform(raw_X_train)\n",
    "nomo_test_transformed = nomogram_preprocessor.transform(raw_X_test)\n",
    "ml_train_transformed = ml_preprocessor.fit_transform(raw_X_train)\n",
    "ml_test_transformed = ml_preprocessor.transform(raw_X_test)\n",
    "\n",
    "###Remove added prefixes in df for readability\n",
    "#ML\n",
    "ml_train_transformed.columns = ml_train_transformed.columns.str.removeprefix(\"num__\")\n",
    "ml_train_transformed.columns = ml_train_transformed.columns.str.removeprefix(\"cat__\")\n",
    "ml_train_transformed.columns = ml_train_transformed.columns.str.removeprefix(\"remainder__\")\n",
    "\n",
    "ml_test_transformed.columns = ml_test_transformed.columns.str.removeprefix(\"num__\")\n",
    "ml_test_transformed.columns = ml_test_transformed.columns.str.removeprefix(\"cat__\")\n",
    "ml_test_transformed.columns = ml_test_transformed.columns.str.removeprefix(\"remainder__\")\n",
    "#NOMO\n",
    "nomo_train_transformed.columns = nomo_train_transformed.columns.str.removeprefix(\"remainder__\")\n",
    "nomo_train_transformed.columns = nomo_train_transformed.columns.str.removeprefix(\"cat__\")\n",
    "nomo_train_transformed.columns = nomo_train_transformed.columns.str.removeprefix(\"bin__\")\n",
    "\n",
    "nomo_test_transformed.columns = nomo_test_transformed.columns.str.removeprefix(\"remainder__\")\n",
    "nomo_test_transformed.columns = nomo_test_transformed.columns.str.removeprefix(\"cat__\")\n",
    "nomo_test_transformed.columns = nomo_test_transformed.columns.str.removeprefix(\"bin__\")\n",
    "\n",
    "#Export scaled/econded dfs to Excel\n",
    "ml_train_transformed.to_excel('./Data/ML_Pipeline/ml_train_transformed.xlsx', index = False)\n",
    "ml_test_transformed.to_excel('./Data/ML_Pipeline/ml_test_transformed.xlsx', index = False)\n",
    "\n",
    "nomo_train_transformed.to_excel('./Data/Nomogram/nomo_train_transformed.xlsx', index = False)\n",
    "nomo_test_transformed.to_excel('./Data/Nomogram/nomo_test_transformed.xlsx', index = False)\n",
    "#Save pipeline for future use\n",
    "joblib.dump(ml_preprocessor, './Data/ML_Pipeline/ml_preprocessing_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "full_X_sm, full_y_sm = smote.fit_resample(ml_train_transformed, raw_y_train)\n",
    "#Reset Index\n",
    "full_X_sm.reset_index(drop = True, inplace=True)\n",
    "full_y_sm.reset_index(drop = True, inplace=True)\n",
    "#Export\n",
    "full_X_sm.to_excel('./Data/ML_Smote/full_x_sm.xlsx', index = False)\n",
    "full_y_sm.to_excel('./Data/ML_Smote/full_y_sm.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM, RF, DT, XGB, KNN, Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOTE: Initial param search space was more broad and spaced at beginning of tuning, an iterative cycle of GridSearchCV \n",
    "####### and manual tuning was used to arrive at final model params\n",
    "\n",
    "###Param Search Grid\n",
    "model_params = {\n",
    "    'SVM': {\n",
    "        'model': svm.SVC(shrinking = True, degree = 7, gamma = 'scale', kernel = 'poly', C=1, class_weight={0:1, 1:3.5}, probability=True), \n",
    "        'params': {\n",
    "            #'class_weight': ['balanced',{0:1, 1:3.15},{0:1, 1:3.25}, {0:1, 1:3.5},{0:1, 1:3.75},{0:1, 1:3.85}],\\\n",
    "            #'C': [0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
    "            #'degree': [6,7,8,9,12],\n",
    "            #'kernel':  ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            #'gamma': ['scale', 'auto'],\n",
    "            #'shrinking': [True, False],            \n",
    "        }\n",
    "    },\n",
    "    'RF': {\n",
    "        'model': ensemble.RandomForestClassifier(criterion = 'log_loss', max_features='sqrt', max_depth = 7, min_samples_split=12,\n",
    "                                                 class_weight = {0:1, 1:3}, n_estimators=22, min_samples_leaf=3),\n",
    "        'params': {\n",
    "            #'class_weight': ['balanced',{0:1, 1:2.75}, {0:1, 1:3}, {0:1, 1:4}],\n",
    "            #'n_estimators': [16,18,20,22]\n",
    "            #'criterion':  [\"entropy\", \"log_loss\"],\n",
    "            #'max_depth': [5,6,7,8,9,10], ##############RETRAIN to make between 5-15#############\n",
    "            #'min_samples_split': [8,9,10,11,12,13],\n",
    "            #'min_samples_leaf': [3,5,6,7],\n",
    "            #'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'DT': {\n",
    "        'model': tree.DecisionTreeClassifier(max_depth = 23, max_features='log2', min_samples_split=3, \n",
    "                                             class_weight = {0:1, 1:4.5}, criterion='entropy', min_samples_leaf=4),\n",
    "        'params': {\n",
    "            #'class_weight': ['balanced', {0:1, 1:4.25}, {0:1, 1:4.5}, {0:1, 1:4.75}],\n",
    "            #'criterion':  [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            #'max_depth': [None, 22, 23,24],\n",
    "            #'min_samples_split': [2,3,4,5,6],\n",
    "            #'min_samples_leaf': [4,5,6,7,8],\n",
    "            #'max_features': ['sqrt', 'log2', None]\n",
    "            }\n",
    "    },\n",
    "    'XGB': {\n",
    "        'model': xgb.XGBClassifier(objective = 'binary:logistic', eval_metric = 'auc', seed = 42, booster = 'dart',\n",
    "                                    grow_policy = 'depthwise', tree_method = 'approx', sampling_method = 'uniform', \n",
    "                                    learning_rate = 0.1, max_depth = 3, scale_pos_weight = 2, min_split_loss =8, \n",
    "                                    min_child_weight = 5, max_delta_step = 0, reg_lambda = 0, reg_alpha = 0),\n",
    "        'params': {\n",
    "            #'booster': ['dart', 'gbtree', 'gblinear'],\n",
    "            #'grow_policy': ['depthwise', 'lossguide'],\n",
    "            #'tree_method': ['auto','approx'],\n",
    "            #'scale_pos_weight': [2,ratio,4,5],\n",
    "            #'learning_rate': [0, 0.1,0.3, 0.5],\n",
    "            #'subsample': [0.5, 0.7, 1], \n",
    "            #'max_leaves': [0,15,20,25], \n",
    "            #'max_bin': [256, 300, 325],\n",
    "            ##Prevent overfitting\n",
    "            #'max_depth': [3,5,6,7],\n",
    "            #'min_child_weight': [1,3,5],\n",
    "            #'min_split_loss': [1,3,5,7,8], \n",
    "            #'max_delta_step': [0, 0.7,1,2,3,4,5],\n",
    "            #'refresh_leaf': [0,1],\n",
    "            #'reg_lambda': [0,2,4,5,6],\n",
    "            #'reg_alpha': [0, 0.5,1,2],\n",
    "            #'sampling_method': ['uniform', 'weighted'], \n",
    "            #'normalize_type': ['tree', 'forest'], \n",
    "            }\n",
    "    },\n",
    "    'KNN':{\n",
    "        'model':  KNeighborsClassifier(algorithm = 'auto', weights = 'distance', metric = 'cosine', p =1, n_neighbors=40),\n",
    "        'params':{\n",
    "            #'n_neighbors': [40,42,43],\n",
    "            #'weights': ['uniform', 'distance'],\n",
    "            #'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            #'metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'nan_euclidean'],\n",
    "            #'leaf_size': [25,30,35], #Only used for BallTree or KDTree algorithm\n",
    "            #'p': [1,2,3],\n",
    "        }\n",
    "    }, \n",
    "    'VOTE': {\n",
    "        'model': ensemble.VotingClassifier(voting = 'soft', estimators = [\n",
    "            ('SVM', svm.SVC(shrinking = True, degree = 7, gamma = 'scale', kernel = 'poly', C=1, class_weight={0:1, 1:3.5}, probability=True)),\n",
    "            ('random_forest', ensemble.RandomForestClassifier(criterion = 'log_loss', max_features='sqrt', max_depth = 7, min_samples_split=12,\n",
    "                                                 class_weight = {0:1, 1:3}, n_estimators=22, min_samples_leaf=3)),\n",
    "            ('XGBoost',xgb.XGBClassifier(objective = 'binary:logistic', eval_metric = 'auc', seed = 42, booster = 'dart',\n",
    "                                    grow_policy = 'depthwise', tree_method = 'approx', sampling_method = 'uniform', \n",
    "                                    learning_rate = 0.1, max_depth = 3, scale_pos_weight = 2, min_split_loss =8, \n",
    "                                    min_child_weight = 5, max_delta_step = 0, reg_lambda = 0, reg_alpha = 0)),\n",
    "            ('Tree', tree.DecisionTreeClassifier(max_depth = 23, max_features='log2', min_samples_split=3, \n",
    "                                             class_weight = {0:1, 1:4.5}, criterion='entropy', min_samples_leaf=4)),\n",
    "            ('KNN',  KNeighborsClassifier(algorithm = 'auto', n_neighbors=40, weights = 'distance', metric = 'cosine', p =1))\n",
    "        ], weights = [5,20,10,0.1,9]),\n",
    "        'params': {\n",
    "            #'voting': ['hard', 'soft'],\n",
    "            #'weights': [[5,15,10,0.1,10], [5,20,10,0.1,9]]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def get_best_params(map_name): #Use CV to get best params on smote dataset\n",
    "    param_map = model_params[map_name]\n",
    "    clf = GridSearchCV(param_map['model'], param_map['params'], scoring = 'roc_auc', cv = 5, return_train_score=True)\n",
    "    clf.fit(full_X_sm, full_y_sm)\n",
    "    print(f'{map_name} Best 5-fold CV Score: {np.round(clf.best_score_, 4)}')\n",
    "    return clf\n",
    "\n",
    "svc_clf = get_best_params('SVM')\n",
    "rf_clf = get_best_params('RF')\n",
    "tree_clf = get_best_params('DT')\n",
    "boost_clf = get_best_params('XGB')\n",
    "knn_clf = get_best_params(\"KNN\")\n",
    "vote_clf = get_best_params('VOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create sub-train and sub-val datasets from train set\n",
    "X_temp, nn_X_val, y_temp, nn_y_val = train_test_split(ml_train_transformed, raw_y_train, test_size = 0.2, stratify = raw_y_train, random_state = 42)\n",
    "#Oversample sub-train set\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "nn_X_sm, nn_y_sm = smote.fit_resample(X_temp, y_temp)\n",
    "\n",
    "#Export to excel\n",
    "nn_X_sm.to_excel('./Data/NN/nn_X_sm.xlsx', index = False)\n",
    "nn_y_sm.to_excel('./Data/NN/nn_y_sm.xlsx', index = False)\n",
    "nn_X_val.to_excel('./Data/NN/nn_X_val.xlsx', index = False)\n",
    "nn_y_val.to_excel('./Data/NN/nn_y_val.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Commented values next to hp vals denote selected values\n",
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape = nn_X_sm.loc[0].shape))\n",
    "\n",
    "    hp_activation = hp.Choice('activation', values = ['relu', 'tanh']) #relu\n",
    "    hp_layer_1 = hp.Int('layer_1', min_value =201, max_value = 401, step = 50) #301\n",
    "    hp_layer_2 = hp.Int('layer_2', min_value =1701, max_value = 1901, step = 50) #1801\n",
    "    hp_layer_3 = hp.Int('layer_3', min_value =1301, max_value = 1501, step = 50) #1401\n",
    "\n",
    "    hp_drop_1 = hp.Choice('drop_1', values = [0.0, 0.2, 0.4, 0.6, 0.8, 0.99999]) #0.8\n",
    "    hp_drop_2 = hp.Choice('drop_2', values = [0.0, 0.2, 0.4, 0.6, 0.8, 0.99999]) # 0.0\n",
    "    hp_drop_3 = hp.Choice('drop_3', values = [0.0, 0.2, 0.4, 0.6, 0.8, 0.99999]) #0.4\n",
    "\n",
    "    hp_lr = hp.Choice('learning_rate', values = [0.005, 0.01, 0.015]) #0.01\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units = hp_layer_1, activation=hp_activation))\n",
    "    model.add(tf.keras.layers.Dropout(hp_drop_1)) \n",
    "    model.add(tf.keras.layers.Dense(units = hp_layer_2, activation=hp_activation))\n",
    "    model.add(tf.keras.layers.Dropout(hp_drop_2))\n",
    "    model.add(tf.keras.layers.Dense(units = hp_layer_3, activation=hp_activation))\n",
    "    model.add(tf.keras.layers.Dropout(hp_drop_3)) \n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=hp_lr),\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = [\n",
    "        tf.keras.metrics.AUC(name='auc_roc'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = kt.Objective('val_auc_roc', 'max'),\n",
    "                     max_epochs = 150,\n",
    "                     factor = 3,\n",
    "                     directory = './nn_tune_dir',\n",
    "                     project_name = 'x')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc_roc', patience = 5)\n",
    "\n",
    "##Tune parameters\n",
    "tuner.search(nn_X_sm, nn_y_sm, \n",
    "             epochs = 150, validation_data = (nn_X_val, nn_y_val),\n",
    "             class_weight={0:1, 1:1.2},\n",
    "             callbacks = [stop_early])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train model with tuned hyper params\n",
    "model_NN = tuner.hypermodel.build(best_hps)\n",
    "history = model_NN.fit(nn_X_sm, nn_y_sm, \n",
    "             epochs = 150,\n",
    "             validation_data = (nn_X_val, nn_y_val),\n",
    "             class_weight={0:1, 1:1.2},\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plot AUROC and loss over epochs\n",
    "# AUROC\n",
    "val_auc_roc = history.history['val_auc_roc']\n",
    "train_auc_roc = history.history['auc_roc']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_auc_roc, label='Validation AUC-ROC')\n",
    "plt.plot(train_auc_roc, label='Train AUC-ROC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('AUC-ROC Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Loss\n",
    "val_loss = history.history['val_loss']\n",
    "train_loss = history.history['loss']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train log model\n",
    "log_model = linear_model.LogisticRegression(max_iter = 6000)\n",
    "log_model.fit(nomo_train_transformed, raw_y_train)\n",
    "\n",
    "##Get coef and intercept from log model\n",
    "coefs = log_model.coef_[0]\n",
    "intercept = log_model.intercept_[0]\n",
    "##Create features and coef columns\n",
    "feature_names = nomo_train_transformed.columns.to_list()\n",
    "features = ['intercept'] + ['threshold'] + feature_names\n",
    "coefs = [intercept] + [0.5] + list(coefs)\n",
    "\n",
    "##Get continuous, nominal, and ordinal columns\n",
    "cat_cols = [0] * 46 ## 0 = numerical\n",
    "for i in range(0,46):\n",
    "    col = nomo_train_transformed.columns[i]\n",
    "    counts = nomo_train_transformed[col].value_counts()\n",
    "    if col in nominal_cols or col in binary_cols: #1 = #Nominal\n",
    "        cat_cols[i] = 1\n",
    "    elif col in ordinal_cols: ##2 = Ordinal\n",
    "        cat_cols[i] = 2\n",
    "\n",
    "##Get mins, maxs, coefs, types, positions columns\n",
    "mins = [''] + [''] +[nomo_train_transformed[feature].min() for feature in feature_names]\n",
    "maxs = [''] + [''] +[nomo_train_transformed[feature].max() for feature in feature_names]\n",
    "coefs = [1e-10 if i == 0 else i for i in coefs] #Make 0 entries a very small number\n",
    "types = [''] +  [''] + ['numerical' if i == 0 else 'nominal' if i ==1 else 'ordinal' for i in cat_cols]\n",
    "positions = [''] * 48\n",
    "\n",
    "##Create table\n",
    "nomo_table = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'coef': coefs,\n",
    "    'min': mins,\n",
    "    'max': maxs,\n",
    "    'type': types,\n",
    "    'position': positions\n",
    "})\n",
    "\n",
    "##Export and import table\n",
    "path_to_nomo_table = './Data/Nomogram/Nomo_Table.xlsx'\n",
    "nomo_table.to_excel(path_to_nomo_table, index = False)\n",
    "\n",
    "##Create nomogram\n",
    "results = nomogram(path = path_to_nomo_table, result_title = 'ORN Risk', \n",
    "                   fig_width=30, single_height=.7, dpi = 500,\n",
    "                   ax_para={'linewidth': 2, 'c': 'black', 'linestyle': '--'},\n",
    "                   xtick_para = {'fontsize': 13}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(model_name, y_test, y_pred):\n",
    "    cm = tf.math.confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sn.heatmap(cm, annot=True, fmt ='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.title(model_name + ' Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def bootstrap_auc(y_true, y_pred, n_bootstraps=3000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    bootstrapped_scores = []\n",
    "    for i in range(n_bootstraps):\n",
    "        # Sample predictions (w/ replcement)\n",
    "        indices = resample(np.arange(len(y_pred)), replace=True)\n",
    "        # ensures resampled data contains both classes\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        # Calculate AUC for resampled data\n",
    "        score = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "    confidence_lower = sorted_scores[int(\n",
    "        0.025 * len(sorted_scores))]  # 2.5th percentile\n",
    "    confidence_upper = sorted_scores[int(\n",
    "        0.975 * len(sorted_scores))]  # 97.5th percentile\n",
    "    return confidence_lower, confidence_upper\n",
    "\n",
    "def auc_CI(y_true, y_pred, n_bootsraps=3000, seed=42):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    CI_lower, CI_upper = bootstrap_auc(y_true, y_pred, n_bootsraps, seed)\n",
    "    return score, CI_lower, CI_upper\n",
    "\n",
    "def evaluate(model, model_name, X_train, y_train, X_test, y_test, include_training = True, threshold = 0.5):   \n",
    "    ########## Get model prob pred and pred based on threshold ########## \n",
    "    model_scores = {}\n",
    "    if (model_name == 'Neural Network'):  # NN doesn't have predict_proba() method\n",
    "        y_pred_proba_test = model.predict(X_test).ravel()\n",
    "        y_pred_proba_train = model.predict(X_train).ravel()\n",
    "    else:\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:,1]\n",
    "    y_pred = (y_pred_proba_test >= threshold).astype('int')\n",
    "\n",
    "    ##################### Get classification report #####################\n",
    "    #Only for testing\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(class_report)\n",
    "    ##################### Get Confusion Matrix #########################\n",
    "    #Only for testing\n",
    "    get_cm(model_name, y_test, y_pred)\n",
    "    ##################### ROC CURVE and AUROC #########################\n",
    "    ##Training\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    if include_training == True:\n",
    "        roc_auc, lower_CI, upper_CI = auc_CI(y_train, y_pred_proba_train)\n",
    "        model_score = f'AUROC = {roc_auc:.3f} ({lower_CI:.3f}-{upper_CI:.3f})'\n",
    "        model_scores[model_name] = model_score\n",
    "        fpr, tpr, thresholds = roc_curve(y_train, y_pred_proba_train)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=4, label=f'Training {model_score}')\n",
    "    ##Testing\n",
    "    roc_auc, lower_CI, upper_CI = auc_CI(y_test, y_pred_proba_test)\n",
    "    model_score = f'AUROC = {roc_auc:.3f} ({lower_CI:.3f}-{upper_CI:.3f})'\n",
    "    model_scores[model_name] = model_score\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=4, label=f'Testing {model_score}')\n",
    "    ####### Plot graphs ######\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 21, fontweight = 550)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 21, fontweight = 550)\n",
    "    plt.tick_params(axis = 'both', which = 'major', labelsize=15)\n",
    "    plt.title(f'{model_name}', fontweight='semibold', fontsize = 25)\n",
    "    plt.legend(loc=\"lower right\", prop = {'size': 21, 'weight': 550})\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get CM, ROC curve, AUROC, CIs for AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(svc_clf, 'SVM', full_X_sm, full_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rf_clf, 'Random Forest', full_X_sm, full_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tree_clf, 'Decision Tree', full_X_sm, full_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(boost_clf, 'XGBoost', full_X_sm, full_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(knn_clf, 'KNN', full_X_sm, full_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(vote_clf, 'Vote', full_X_sm, full_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model_NN, 'Neural Network', nn_X_sm, nn_y_sm, ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(log_model, 'Nomogram', nomo_train_transformed, raw_y_train, nomo_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcurve(model, model_name, X_test, y_test):\n",
    "    #Get predictions\n",
    "    if (model_name == 'Neural Network'):  # Different pred method for NN\n",
    "        y_pred_proba_test = model.predict(X_test).ravel()\n",
    "    else:\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    #Create new df with added predicted prob of ORN and ORN occurance cols\n",
    "    df = X_test.copy()\n",
    "    df['Probability'] = y_pred_proba_test\n",
    "    df['ORN'] = y_test\n",
    "    \n",
    "    results = dca(\n",
    "        data=df,\n",
    "        outcome='ORN',\n",
    "        modelnames=['Probability'],\n",
    "        thresholds=np.arange(0, 1, 0.05)\n",
    "    )\n",
    "    \n",
    "    plot_graphs(\n",
    "        plot_df = results,\n",
    "        y_limits = [-0.05, 0.4],\n",
    "        graph_type='net_benefit'\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get decision curves using testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(rf_clf, 'Random Forest', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(tree_clf, 'Decision Tree', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(boost_clf, 'XGBoost', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(knn_clf, 'KNN', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(vote_clf, 'Vote', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(model_NN, 'Neural Network', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(model_NN, 'Neural Network', ml_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_dcurve(log_model, 'Nomogram', nomo_test_transformed, raw_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: combine_encoded() function taken from following repository:\n",
    "# https://gist.github.com/peterdhansen/ca87cc1bfbc4c092f0872a3bfe3204b2#\n",
    "\n",
    "######## Combine one-hot-encoded #######\n",
    "def combine_encoded(shap_values, name, mask, return_original=True):\n",
    "    mask = np.array(mask)\n",
    "    mask_col_names = np.array(shap_values.feature_names, dtype='object')[mask]\n",
    "    sv_name = shap.Explanation(shap_values.values[:, mask],\n",
    "                               feature_names=list(mask_col_names),\n",
    "                               data=shap_values.data[:, mask],\n",
    "                               base_values=shap_values.base_values,\n",
    "                               display_data=shap_values.display_data,\n",
    "                               instance_names=shap_values.instance_names,\n",
    "                               output_names=shap_values.output_names,\n",
    "                               output_indexes=shap_values.output_indexes,\n",
    "                               lower_bounds=shap_values.lower_bounds,\n",
    "                               upper_bounds=shap_values.upper_bounds,\n",
    "                               main_effects=shap_values.main_effects,\n",
    "                               hierarchical_values=shap_values.hierarchical_values,\n",
    "                               clustering=shap_values.clustering,\n",
    "                               )\n",
    "    new_data = (sv_name.data * np.arange(sum(mask))).sum(axis=1).astype(int)\n",
    "    svdata = np.concatenate([\n",
    "        shap_values.data[:, ~mask],\n",
    "        new_data.reshape(-1, 1)\n",
    "    ], axis=1)\n",
    "\n",
    "    if shap_values.display_data is None:\n",
    "        svdd = shap_values.data[:, ~mask]\n",
    "    else:\n",
    "        svdd = shap_values.display_data[:, ~mask]\n",
    "\n",
    "    svdisplay_data = np.concatenate([\n",
    "        svdd,\n",
    "        mask_col_names[new_data].reshape(-1, 1)\n",
    "    ], axis=1)\n",
    "\n",
    "    new_values = sv_name.values.sum(axis=1)\n",
    "    svvalues = np.concatenate([\n",
    "        shap_values.values[:, ~mask],\n",
    "        new_values.reshape(-1, 1)\n",
    "    ], axis=1)\n",
    "    svfeature_names = list(np.array(shap_values.feature_names)[~mask]) + [name]\n",
    "\n",
    "    sv = shap.Explanation(svvalues,\n",
    "                          base_values=shap_values.base_values,\n",
    "                          data=svdata,\n",
    "                          display_data=svdisplay_data,\n",
    "                          instance_names=shap_values.instance_names,\n",
    "                          feature_names=svfeature_names,\n",
    "                          output_names=shap_values.output_names,\n",
    "                          output_indexes=shap_values.output_indexes,\n",
    "                          lower_bounds=shap_values.lower_bounds,\n",
    "                          upper_bounds=shap_values.upper_bounds,\n",
    "                          main_effects=shap_values.main_effects,\n",
    "                          hierarchical_values=shap_values.hierarchical_values,\n",
    "                          clustering=shap_values.clustering,\n",
    "                          )\n",
    "    if return_original:\n",
    "        return sv, sv_name\n",
    "    else:\n",
    "        return sv\n",
    "    \n",
    "\n",
    "def get_shap(model, model_name, X_train, show_initial = False, show_sub_plots = False, display_df = False):\n",
    "    if model_name == 'Neural Network': # Different method for neural network\n",
    "        explainer = shap.Explainer(model, X_train)\n",
    "    else:\n",
    "        explainer = shap.Explainer(model.predict, X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    ##Show initial plot without combining one-hot-encoded\n",
    "    if show_initial == True:\n",
    "        shap.plots.beeswarm(shap_values, max_display=45, show=False)\n",
    "        plt.title(f'{model_name} Raw SHAP Plot')\n",
    "        plt.show()\n",
    "\n",
    "    #### Combine JEWER ######\n",
    "    shap_vals_jewer, sv_occ_jewer = combine_encoded(shap_values, 'Jewer-Boyd classification', \n",
    "                                              ['JEWER' in n for n in shap_values.feature_names])\n",
    "    #### COMBINE SITE #######\n",
    "    shap_vals_site, sv_occ_site = combine_encoded(shap_vals_jewer, 'Tumor site', \n",
    "                                             ['SITE' in n for n in shap_vals_jewer.feature_names])\n",
    "    ###### COMBINE PLATE #######\n",
    "    shap_vals_plate, sv_occ_plate = combine_encoded(shap_vals_site, 'Type of plate', \n",
    "                                              ['PLATE' in n for n in shap_vals_site.feature_names])\n",
    "    #####COMBINE TXEXPOSURE #####\n",
    "    shap_vals_tx, sv_occ_tx = combine_encoded(shap_vals_plate, 'Non-medication for plate exposure', \n",
    "                                              ['TXEXPOSURE' in n for n in shap_vals_plate.feature_names])\n",
    "    ######SHOW SUB_PLOTS########\n",
    "    if show_sub_plots == True:\n",
    "        ##JEWER\n",
    "        shap.plots.beeswarm(sv_occ_jewer, max_display=20, show=False)\n",
    "        plt.title(f'{model_name} Jewer SHAP Plot')\n",
    "        plt.show()\n",
    "        ##SITE\n",
    "        shap.plots.beeswarm(sv_occ_site, max_display=20, show=False)\n",
    "        plt.title(f'{model_name} Site SHAP Plot')\n",
    "        plt.show()\n",
    "        ##PLATE\n",
    "        shap.plots.beeswarm(sv_occ_plate, max_display=20, show=False)\n",
    "        plt.title(f'{model_name} Plate SHAP Plot')\n",
    "        plt.show()\n",
    "        ##TXEXPOSURE\n",
    "        shap.plots.beeswarm(sv_occ_tx, max_display=20, show=False)\n",
    "        plt.title(f'{model_name} TXEXPOSURE SHAP Plot')\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    ############Get shap values, mean shap, sum shap########\n",
    "    shap_array = shap_vals_tx.values\n",
    "    feature_names = shap_vals_tx.feature_names\n",
    "    shap_df = pd.DataFrame(shap_array, columns=feature_names)\n",
    "    #Absolute AVG shap vals for each feature\n",
    "    absolute_mean_shap = shap_df.abs().mean().reset_index()\n",
    "    absolute_mean_shap.columns = ['Feature', 'Mean Absolute SHAP Value']\n",
    "    if display_df == True:\n",
    "        display(absolute_mean_shap)\n",
    "    #Plot beeswarm plot\n",
    "    shap.plots.beeswarm(shap_vals_tx, max_display=35, show=False)\n",
    "    plt.title(f'{model_name}',  fontweight='semibold', fontsize = 25)\n",
    "    plt.xlim(-0.7, 0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use training data to get SHAP Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(svc_clf, 'SVM', full_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(rf_clf, 'Random Forrest', full_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(tree_clf, 'Decision Tree', full_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(boost_clf, 'XGBoost', full_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(knn_clf, 'KNN', full_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(vote_clf, 'Vote', full_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(model_NN, 'Neural Network', nn_X_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap(log_model, 'Nomogram', nomo_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-vals for AUROCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: DeLong's test in R (using RStudio) from the pROC package was used to obtain p-values. Use the following code to get model predictions. Export these predictions and use the R code to get p-vals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------TRUE y-----------------\")\n",
    "display(raw_y_test.to_list())\n",
    "##predict_proba() returns 2-d array, second entry is predicted prob of ORN occurance\n",
    "y_proba_svm = svc_clf.predict_proba(ml_test_transformed)[:,1]\n",
    "print('-----------------SVM----------------')\n",
    "display(y_proba_svm)\n",
    "\n",
    "y_proba_rf = rf_clf.predict_proba(ml_test_transformed)[:,1]\n",
    "print('----------------RF----------------')\n",
    "display(y_proba_rf)\n",
    "\n",
    "y_proba_tree = tree_clf.predict_proba(ml_test_transformed)[:,1]\n",
    "print('----------------TREE----------------')\n",
    "display(y_proba_tree)\n",
    "\n",
    "y_proba_boost = boost_clf.predict_proba(ml_test_transformed)[:,1]\n",
    "print('----------------BOOST----------------')\n",
    "display(y_proba_boost)\n",
    "\n",
    "y_proba_knn = knn_clf.predict_proba(ml_test_transformed)[:,1]\n",
    "print('----------------KNN----------------')\n",
    "display(y_proba_knn)\n",
    "\n",
    "y_proba_vote = vote_clf.predict_proba(ml_test_transformed)[:,1]\n",
    "print('----------------VOTE----------------')\n",
    "display(y_proba_vote)\n",
    "#NN predict method only returns 1D array for prob of ORN\n",
    "y_proba_nn = model_NN.predict(ml_test_transformed).ravel() \n",
    "print('----------------NN----------------')\n",
    "display(y_proba_nn)\n",
    "y_proba_nomo = log_model.predict_proba(nomo_test_transformed)[:,1]\n",
    "print('----------------NOMO----------------')\n",
    "display(y_proba_nomo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orn_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
